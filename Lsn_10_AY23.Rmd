---
title: "Lesson 10 AY23"
author: "Clark"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

Recall that earlier in the course we discussed confounding, which is

\vspace{1.5in}

In today's lesson (which adittedly is a bit dense) we are going to go through how confounding can make life difficult and impact our analysis of variance model.

The primary research quesiton we are going to explore is whether wages for blacks differ significantly fromw ages for non-boacks focusing on males who went to college and males who did not go to college.

The initial statistical model we consider is:

\vspace{1.in}

We can find the group means by:

```{r}
dat<-read.table("http://www.isi-stats.com/isi2/data/WageSubset.txt",header=T)
dat %>% group_by(race,education)%>%summarise(avg=mean(wage.100))
mean(dat$wage.100)
```

Note here that the overall mean is a lot closer to nonblack than it is to black.  Why?

\vspace{.4in}

Therefore we might not want $\mu$ in our model to represent the overall average, but rather the average of the group averages, or $(4.52+6.21)/2$.  In R this is done when we fix our contrasts as contr.sum
```{r}
dat<-read.table("http://www.isi-stats.com/isi2/data/WageSubset.txt",header=T)
dat <- dat %>% mutate(race=as.factor(race))%>%
  mutate(education = as.factor(education))
contrasts(dat$race)=contr.sum
contrasts(dat$education)=contr.sum
anova_model2<-lm(wage.100~race,data=dat)
full.bets<-anova_model2$coefficients
full.bets
```

Again $\mu$ is NOT the population average, but the \textbf{effect average}.  Why might we want to do this

\vspace{1.5in}

Looking at page 175 obviously we might want to explain some of the unexplained variation using college as a factor.  The real issue becomes this:

```{r}
dat %>% group_by(race,education)%>%summarise(num.obs=n())
```

So let's do what we did before while ignoring the fact that our samples are unequal.
```{r}

dat<-read.table("http://www.isi-stats.com/isi2/data/WageSubset.txt",header=T)
dat <- dat %>% mutate(race=as.factor(race))%>%
  mutate(education = as.factor(education))
dat %>% group_by(education)%>%summarise(avg=mean(wage.100))
```

Therefore the  means of the means is 7.477 and the effect of education is $\pm 2.181$.  So perhaps we are tempted to our adjusted statistical model as:


\vspace{.5in}


Which we could then analyze via:

```{r}
dat.adj = dat %>% mutate(adj.val=ifelse(education=="belowCollege",wage.100+2.181,wage.100-2.181))

adj.mod<-lm(adj.val~race,data=dat.adj)

anova(adj.mod)
```

Which seems like it should work, right?  This is just what we were doing before, what's the problem?

\vspace{1.in}

This is, in essence, confounding.  When we subtract off the "College effect" we are also subtracting off some part of the race effect.  Why?

\vspace{1.in}

In the parlance of ANOVA, up to this point we have been calculating what are called "Type I Sums of Squares".  These are done sequentally.  We first find the Sums of Squares due to factor A and then find the Sums of Squares due to factor B given than factor A is in the model.  We can see this because if we run:

```{r}
forward<-lm(wage.100~race+education,data=dat)
anova(forward)
```


```{r}
backward<-lm(wage.100~education+race,data=dat)
anova(backward)
```

Our Sums of Squares change.  \textbf{This is because the only time we are doing "Conditional Sums of Squares" is when our variable is the second variable in the model}

To further see that education and race are covariated, we note that by knowing someone's education we have information on race.  Further, by knowing education we have information on wage. 


\vspace{.5in}

To reflect covariance in our model we draw our diagram like:

\vspace{1.5in}

Note that our statistical model doesn't change, but to fit this in R we need the `library(car)` installed and we can run:
```{r,message=FALSE,warning=FALSE}
library(car)
contrasts(dat$race)=contr.sum
contrasts(dat$education)=contr.sum
anova_model2<-lm(wage.100~education+race,data=dat)
anova.table<-Anova(anova_model2,type=3)
anova.table
```

An interesting note here is that the sums of squares no longer equal the total sums of squares.  The extra sums of squares can be thought of as variation that cannot be disentangled from education or race.  Our book calls this SScovariation, which I rather like.  It's variability that still exists but we cannot attribute to either factor so we basically shrug our shoulders.


\newpage

Type I Sums of Squares vs Type III Sums of Squares

- Type I Calculations Make Sense

\vspace{1.5in}

- Type I prefereable when order matters (More concerned with one of the factors First)

\vspace{1.5in}

- Type I Order Matters!

\vspace{.5in}

- Type III all effects are conditional on \textit{everything else in the model}

\vspace{1.in}

- Type III not sample size dependent

- Type III are NOT additive



