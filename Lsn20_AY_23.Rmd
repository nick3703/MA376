---
title: "Lsn 20_AY_23"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=12, fig.height=6,fig.show = "hide")
library(tidyverse)
library(GGally)
```

## Admin

Recall that the general form for a linear regresison model is:

\vspace{1.in}

We can think about this model as having two components, a linear predictor $\beta_0+\beta_1 x_{1,i}+\beta{2}x_{2,i}+\cdots+\beta{n}x_{n,i}$ and a random mechanism that perturbs us from the plane, $\epsilon_i$.

However, sometimes, we might believe that the relationship between our covariates and response variable is not linear.  This can be done for a few different reasons, perhaps the best reason is if we have some previous knowledge that suggests $x$ and $y$ have a nonlinear relationship.  For example, if we remember the differential equation for radio active decay we had:

$$Y'(t)=-k Y(t)$$

Which yielded a solution of:

$$Y(t)=C e^{-k t}$$

Where $C$ depended on the initial conditions, say $C=3$ was the initial amount of substance.  Then we had a non linear relationship between $Y$ and $t$.  If we measured time and amount of radioactive subtstance we would have to account for measurement error and perhaps we could fit the statistical model:

\vspace{1.in}

Therefore it might make sense to talk about a general form for statistical models of:

\vspace{1.in}

This model has two components, signal, and noise.  While the best case is we are using a scientific mechanism to define the form of $f(x_i)$, in other cases we might just observe that clearly $x_i$ and $y_i$ don't have a linear relationship, so we might explore other forms of $f(x_i)$.

The simplest function outside of a linear relationship is if we assume $f(x_i)=\beta_0+\beta_1 x_{1,i}+\beta_2 x_{1,i}^2$.  Or $x_{1,i}$ and $y_i$ have a quadratic relationship.  This is, what our text calls, a \textbf{polynomial statistical  model}.  While the relationship between $x_{1,i}$ and $y_i$ is non-linear here, fitting the model can be achieved in the exact same way as a linear regression model.  To see this, let's consider the Kentucky Derby data

```{r}
ky.dat<-read.table("http://www.isi-stats.com/isi2/data/KYDerby18.txt",header=T,stringsAsFactors = T)
ky.dat %>% ggplot(aes(x=Year,y=Time))+geom_point()
```

That's kinda weird...  But as it turns out, the distance changed in 1896, so we're comparing apples to oranges.  Let's look at speed vs year

```{r}
ky.dat %>% ggplot(aes(x=Year,y=speed))+geom_point()
```

Is there a story to the data?  Unusual observations?

\vspace{1.in}

This isn't uncommon in athletic performace.  We might think about there being a cap on the fastest a horse can possibly run.  So it might make sense to fit a quadratic model.  The model will be:

\vspace{1.in}

We can fit this by:
```{r}
ky.dat <- ky.dat %>% mutate(Year.sq=Year^2)
poly.lm<-lm(speed~Year+Year.sq,data=ky.dat)
summary(poly.lm)
```

To check the fit we can look at:

```{r}
ky.dat %>% ggplot(aes(x=Year,y=speed))+geom_point()+
  geom_line(aes(x=Year,y=.fitted),data=poly.lm,lwd=2,color="red")
```

Fit looks decent.

To check assumption on $\epsilon_i$ we have:

```{r}
poly.lm %>% ggplot(aes(x=.fitted,y=.resid))+geom_point()
```

Any concerns?

\vspace{1.in}

The fitted or predicted model is:

\vspace{1.in}

If we want to predict from this model, we could do:

```{r}
predict(poly.lm,data.frame(Year=2019,Year.sq=2019^2),interval="prediction")
```

Let's look at this:

```{r}
pred.df<-data.frame(y1=35.4,y2=37.8,x1=2019,x2=2019)
ky.dat %>% ggplot(aes(x=Year,y=speed))+geom_point()+
  geom_line(aes(x=Year,y=.fitted),data=poly.lm,lwd=2,color="red")+
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),lwd=2,colour = "blue", data = pred.df)
```

One concern we might have is colinearity or a relationship between our predictors.  To examine this we again look at a pairs plot:

```{r}
sub.df<-data.frame(speed=ky.dat$speed,Year=ky.dat$Year,Year.sq=ky.dat$Year.sq)
ggpairs(sub.df)
```

To fix this issue we can use what are called Orthogonal Polynomials.  The scope of this is a bit beyond the course, but how this works is, our intercept is equal to 1, our first polynomial is equal to $x\equiv x-\bar{x}$.

The second polynomial is found via recursion:

$$P_2(x) = (x-\frac{<x^2,x>}{<x,x>})x - \frac{<x,x>}{n}$$
Each of the polynomial terms are then scaled by their l2 norm making them orthonormal polynomials.

Once this is done, these new covariates retain the polynomial shape of the raw polynomials, but are now uncorrelated with each other.  This is done in R using `poly()`

```{r}
orth.lm<-lm(speed~poly(Year,2),data=ky.dat)
summary(orth.lm)
ky.dat %>% ggplot(aes(x=Year,y=speed))+geom_point()+
    geom_line(aes(x=ky.dat$Year,y=.fitted),data=orth.lm,lwd=2,color="red")
```

Fit is the exact same

```{r}
predict(orth.lm,data.frame(Year=2019,Year.sq=2019^2),interval="prediction")
```

Prediction is the same

```{r}
new.df<-data.frame(speed=ky.dat$speed,v1=model.matrix(orth.lm)[,2],v2=model.matrix(orth.lm)[,3])
ggpairs(new.df)
```

Our book discusses standardizing our covariates which does something similar, but orthogonal polynomials are probably more common in practice.  We can also add a cubic term to the model:

```{r}
ky.dat <- ky.dat %>% mutate(Year.3=Year^3)
poly3.lm<-lm(speed~Year+Year.sq+Year.3,data=ky.dat)
```

Does this appear to significantly improve the fit?

```{r}
orth3.lm<-lm(speed~poly(Year,3),data=ky.dat)
summary(orth3.lm)
```

If we want to account for track condition we note that there's a ton of levels:

```{r}
levels(ky.dat$condition)
```

After adjusting for year, we could fit:
```{r}
condition.lm<-lm(speed~poly(Year,2)+condition,data=ky.dat)
summary(condition.lm)
```

To see if this matters we can compare the smaller (nested) model via:

```{r}
anova(poly.lm,condition.lm)
```



So, again, here we have been talking about models like:

\vspace{.5in}

Where the $f(x_i)$ that we considered was the class of polynomial functions.  Another class of regression models is fitting:

\vspace{1.in}

Why would we want to do this?  Well, we might do it because we have a reason to believe that this is the underlying relationship.  Another common reason is that our assumptions are violated.  For instance, speed and stopping distance look like:

```{r}
stopping.dat<-read_table("http://www.isi-stats.com/isi2/data/stopping.txt")%>%drop_na()
stopping.dat %>% ggplot(aes(x=speed,y=stoppingdistance))+
  geom_point()
```

Here we see that a model relating speed to stopping distane linearly might not be appropriate.  Why?

\vspace{1.in}

To account for this we might consider transforming $y$.  One common transformation is $\log(y)$  (Note:  From here on out in life, when someone writes $\log$ assume that it is natural log)

Why $\log(y)$?  This comes from the delta method in statistics. The assumption we are making when we use the log transformation is that the variance of $y$ is increasing as the expected value of $y$ increases.  That is:

\vspace{1.in}

The delta method says that any transformation of a random variable can be expressed as:

\vspace{1.in}

Similarly $\sqrt{y}$ can be used if we have $y$ that has variance of:

\vspace{1.in}

So, any power transformation can be done if we want to correct or stabilize our variance and our variance is a function of our mean.

So why not transform?

Well, it changes the relationship between $x_i$ and $y_i$.  Note that previously we had assumed $x_i$ and $y_i$ had a linear relationship.  Now, let's take $\log(y_i)$ and fit a regression model.  The model is now:

\vspace{1.in}

So now the relationship between $x_i$ and $y_i$ becomes:

\vspace{1.in}

Overall, my recommendation is, if I don't care about exploring a linear relationship between $x_i$ and $y_i$ then transform away in order to fix assumptions.  If we DO care about the linear relationshp, then we shouldn't do this.

Here we could do:
```{r}
stopping.dat=stopping.dat %>% mutate(log.stop=log(stoppingdistance))
log.lm<-lm(log.stop~speed,data=stopping.dat)

log.lm %>% ggplot(aes(x=.fitted,y=.resid))+geom_point()
```

If we are satisfied with this we could find:
```{r}
predict(log.lm,data.frame(speed=4),interval="prediction")
```

But remember that this isn't a PI for stopping.  It is for log(stopping), so our 95 \% PI for stopping is 

```{r}
exp(1.1)
exp(2.6)
```

Let's see what happens though if we choose a different transformation:

```{r}
stopping.dat=stopping.dat %>% mutate(sq.stop=sqrt(stoppingdistance))
sq.lm<-lm(sq.stop~speed,data=stopping.dat)

sq.lm %>% ggplot(aes(x=.fitted,y=.resid))+geom_point()
```

Fit looks better

```{r}
predict(sq.lm,data.frame(speed=4),interval="prediction")
```

Again, this is for square root of stopping time, so to find actual interval we need:
```{r}
.452^2
3.4^2
```

So certainly the transformation matters.  How do we know that we have the \textit{right} transformation?  Well, we cannot do:

```{r}
anova(sq.lm,log.lm)
```

In fact it gives us a warning.  Our models are NOT nested, so statistics doesn't help us here.  

One way is to note that all of the transformations are special cases of what are known as Box-Cox transformations.

\vspace{1.in}

So we can find the value of $\lambda$ that maximizes the log-likelihood of this.

```{r}
library(MASS)
boxcox(stoppingdistance~speed,data=stopping.dat)

```

Here $\lambda=0$ is log transformation and $\lambda=.5$ is square root, $\lambda=1$ is no transformation.  So it looks like $\lambda \approx .5$ would be preferable here.

So the model would be:

\vspace{.5in}

Note here there is not a nice clean linear or multiplicative relationship between $x_i$ and $y_i$, but there is \textbf{a} relationship.  If we want a predictive model this would suffice but if we want to explore a linear relationship between $x_i$ and $y_i$ this would not be appropriate.