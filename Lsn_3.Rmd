---
title: "Lesson 2"
author: "Nicholas Clark"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE,results="show",fig.show="show")
```

## Admin

\vspace{.3in}

The begining of our text focuses on experiments vs. observational studies.  Why is this important?

\vspace{.3in}

At West Point, as well as at most universities, prior to conducting an experiment, your \textbf{study protocol} must be reviewed by an Institutional Review Board or IRB.  The point of the IRB is to protect the rights of the subjects of a study as well as to ensure that inferences made from the study are statistically valid.

A \textbf{double blind} study is:

\vspace{.4in}

Why is this important?

\vspace{.5in}

Our book talks about a study on store ratings and wants to determine whether a rating is influenced by exposure to a scent.  Are there ethical issues with this study?

\vspace{.3in}


The first model they consider is

\begin{align*}
i &= \mbox{ Student}\\
y_i & = \mbox{rating of student }i\\
y_i & = \mu + \epsilon_i
\end{align*}

What is the sources of variation diagram associated with this model?

\vspace{2.in}


What does $\epsilon_i$ represent in this model?

\vspace{1.3in}

Are there any assumptions we are making on $\epsilon_i$?

\vspace{.5in}

The book says that the fitted model is:

\begin{align*}
y_i &= 4.48 + \epsilon_i\\
\epsilon_i & \sim F(0,1.27)
\end{align*}

Note here I use the generic $F$ to stand for some distribution, I'm not making any distributional assumptions on $\epsilon_i$.  How did the book find $\hat{\mu}=4.48$ and the standard error of the residiuals as 1.27?

\vspace{1.5in}

What assumption are we making when we use this model?  What would our causal diagram look like?

\vspace{.5in}

What is the treatment variable?  Let's sketch out the sources of variation diagram

\vspace{1.4in}

Our proposed diagram is:

\vspace{1.in}

We can visualize:

```{r,warning=FALSE,message=FALSE}
library(tidyverse)

dat=read.table("http://www.isi-stats.com/isi2/data/OdorRatings.txt",header=T)

dat %>% ggplot(aes(x=condition, y=rating,fill=condition)) + 
  geom_violin(trim = FALSE)+
  geom_dotplot(binaxis='y', stackdir='center')+
  coord_flip()
```


A statistical model that could be used to address the scientific question is:

\vspace{1.5in}


How could we fit this model?  Well, getting the estimates for $\mu_1$ and $\mu_2$ shouldn't be hard.

```{r}
dat %>% group_by(condition)%>%summarize(samp.mus=mean(rating),sds=sd(rating))

```


```{r}

scent.model=lm(rating~0+condition,data=dat)
summary(scent.model)
```

Note that the standard error from this output does not match the standard error given on the top of page 39.  Why do you think that is?  How could we match the standard error given on page 39?

\vspace{1.5in}

Looking at the output, (ignoring p values for now), what appears to be happening?  How certain are we?  How could we be sure?

\vspace{.5in}

What could be a confounding variable for this study?

\vspace{.5in}

The most important part of thinking of confounding is given in figure 1.1.5.

\vspace{.5in}

This is in our text, but it bears repeating: The goal of random assignment is to reduce the chances of there being any confounding variables in the study. By creating groups that are expected to be similar with respect to all variables (other than the treatment variable of interest) that may impact the response, random assignment attempts to eliminate confounding. A key consequence of not having variables confounded with the treatment variable in a randomized experiment is the potential to draw cause-and-effect conclusions between the treatment variable and the response variable.


https://www.vox.com/science-and-health/2018/6/20/17464906/mediterranean-diet-science-health-predimed

## Think - If our investigators wanted to know if there was a difference between `scent` and `noscent` what would we be testing in terms of our parameters?


