---
title: "Lesson 17 AY23"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=5,fig.height=3,warning = FALSE,message = FALSE)
library(tidyverse)
```

## Admin

Recall that a simple linear regression model is written as:

\vspace{1.in}

And it explains how the variability in a quantitative outcome is explained through a quantitative explanatory variable.  Often times though, we have multiple explanatory variables.  For instance, we previously had models where we had two categorical explanatory variables:

\vspace{1.in}

But let's consider housing prices.  It makes sense that the variability in prices could be explained through the square footage of the house.  Why couldn't we use an ANOVA model for this?

\vspace{.5in}

Using square footage we could write:

\vspace{.5in}

Fitting the model is then done through:

```{r}
house.dat<-read.table("http://www.isi-stats.com/isi2/data/housing.txt",header=T,stringsAsFactors = T)
house.dat<- house.dat %>% mutate(price=price.1000)%>% select(-price.1000)
sqft.lm<-lm(price~sqft,data=house.dat)
summary(sqft.lm)
anova(sqft.lm)

house.dat <- house.dat %>% mutate(lake=location)
```

What are we testing with the two tests above?


\vspace{.5in}

Remember here the choices of using an ANOVA test or using the output of the linear regression model is a choice in using the F statistic or the $\hat{\beta}_1$ statistic.  Either way, we have validity conditions.

```{r}
fit.house <- house.dat %>% mutate(resids=sqft.lm$residuals,preds=sqft.lm$fitted.values)
fit.house %>% ggplot(aes(x=preds,y=resids,color=lake))+geom_point()
```

What do we notice here?  Are we concerned?

\vspace{.5in}

If we want to see the effect of being a lakefront house or the effect of being a not lake front we could calculate

```{r}
fit.house %>% group_by(lake)%>%summarize(mean=mean(resids))
```

However, our analysis is still not entirely straight forward, because if we know whether a house is lake front or not lakefront do we know anything about the square footage of the house?

```{r}
fit.house %>% group_by(lake)%>%summarize(mean=mean(sqft))
```

Note here I'm going to deviate slightly from the text.  One way to adjust for one of the vairables that's in our model is to consider the statistical model for lakefront and price:

\vspace{.5in}

If we want to adjust for lakefront we would do:

\vspace{.5in}
```{r}
contrasts(house.dat$lake)=contr.sum
lake.lm<-lm(price~lake,data=house.dat)
coef(lake.lm)
```

Thus we can adjust by adding 197.2 to every nonlakefront house and subtracting 197.2 to every lakefront house

```{r}
house.dat.mod<-house.dat%>%mutate(price=ifelse(lake=="lakefront",price-197.2,price+197.2))

mod.lm<-lm(price~sqft,data=house.dat.mod)
coef(mod.lm)
```

Note that this is slightly different then what the book gives, the reason here is the meaning of $\mu$ vs $\beta_0$ when we have unbalanced design.  Recall that when we were unbalanced $\mu$ wasn't the overall mean, but rather the mean of the means.  This means when we built our model above and subtracted off the effects of lakefront or not lakefront we are left with $\mu+\epsilon_{i,j}$, which is fine, but that $\mu$ isn't the $\mu$ that we want for square footage...

So let's do this another way.  Instead of subtracting off just the effects, let's subtract off everything except the unexplained variation.

```{r}
house.dat.mod<-house.dat%>%mutate(lake.adj.price=lake.lm$residuals)
mod.lm<-lm(lake.adj.price~sqft,data=house.dat.mod)
coef(mod.lm)
```
So not quite what our book has, but if we now add back $\mu$, which is 408 to the intercept we get 110, which is what our book has.

The bottom line is this, to adjust for effects, fit a model and regress the new, additional variable on the residuals.  A plot of this is called and added variable plot and is implimented in `library(car)` using `avPlot` 
```{r}
library(car)
full.lm<-lm(price~lake+sqft,house.dat)
avPlots(full.lm)
```

Note that these values are centered, which we'll talk about later, but the bottom line is we are adjusting both square feet and price by lake effect and determining whether after accounting for lake effect is there still a relationship between square feet and price.  As our book points out, these are useful if you want to visually explore whether a new explanatory variable explains additional variation.

Here we might decide that square feet does explain variation, so it makes sense to write a new model as:

\vspace{1.in}

Up to now we have been using effect coding as is natural for ANOVA, this was done by setting `contrasts(house.dat$lake)=contr.sum` when we do this we are saying $x_{2,i}=-1$ if observation $i$ is lake front, $-1$ otherwise.  Naturally R uses indicator variables instead, $x_{2,i}=1$ if lakefront, $0$ otherwise.  As explained previously, it doesn't really matter.  Here we'll stick with effect coding.

```{r}
summary(full.lm)
```
What are we testing here?

\vspace{.5in}

What is the fitted model for lakefront?

\vspace{.5in}

What is the fitted model for not lakefront?

\vspace{.5in}

As we see in the fitted models what we have essentially done is fit two lines with the same slope but different intercepts
or our home prices, what we are saying with our models is that the relationship between square footage and price is the same for both lake front and not lake front houses, but the baseline cost differs.  To put itn another way, if we have two houses, one on the lakefront and one not on the lakefront, the difference between the two prices is always expected to be:

\vspace{.5in}

Therefore, if we took, say a 1500 square foot house on the lakefront and a 1500 square foot house not on the lakefront the expected difference is the same as the difference between a 3000 square foot house on the lakefront and a 3000 square foot house not on the lakefront.  Let's look at a picture:

```{r}
house.dat %>% ggplot(aes(x=sqft,y=price,color=lake))+geom_point()+
  stat_smooth(method="lm",se=FALSE,fullrange=T)
```

Here we decided to draw separate regression lines for each group (Note the `group_by` command prior to plotting).  What can we say about the difference of the differences?

\vspace{.5in}

This suggests the possibilities of an interaction model.  We can write the model as:

\vspace{1.in}

In R we fit it as:

```{r}
contrasts(house.dat$lake)=contr.sum
inter.lm<-lm(price~sqft+lake+sqft:lake,data=house.dat)
summary(inter.lm)
```

Let's use this to write out the fitted model for lakefront:

\vspace{.5in}

Fitted model for not lakefront:

\vspace{.5in}

Note we also can write the model as:

\vspace{1.in}

Which can be fit as:

```{r}
contrasts(house.dat$lake)=contr.treatment
inter.lm<-lm(price~sqft+lake+sqft:lake,data=house.dat)
summary(inter.lm)
```

Verify that these outcomes yield the same fitted model.

Note that for either output we get an F-statistic of 91.84 on 3 and 9 DF.  This is testing:

\vspace{.5in}

This probably isn't the test we want in this case.  Note that we also have P values associated with sqft, lake2, and sqft:lake2.  These are testing:

\vspace{1.in}

Note that this is very similar to using Type III sums of squares.  Alternatively we can use the F statistic to test the effects using


```{r}
library(car)
Anova(inter.lm,type=3)
```

Let's tear this apart a bit.  One question you might have is, if we are doing this analysis and end up with different slopes and different intercepts, why not just split our data into two and fit two different regression models?

Certainly we could do:

```{r}
lakehouses<-house.dat %>% filter(lake=="lakefront")
lake.lm<-lm(price~sqft,data=lakehouses)
summary(lake.lm)

```

Which yields a fitted model of:

\vspace{1.in}

And we could do:
```{r}
nonlakehouses<-house.dat %>% filter(lake!="lakefront")
nonlake.lm<-lm(price~sqft,data=nonlakehouses)
summary(nonlake.lm)

```

Why might we not want to do this?

\vspace{1.in}

We still need to check our assumptions, what plots would we want to examine?

\vspace{.5in}

Note that our book makes a statement that may get hidden, but it is actually quite powerful and gets misinterpreted quite a bit.  Note that our confidence intervals for our regression coefficients can be found from:

```{r}
confint(inter.lm)
```

These intervals are NOT confidence intervals for $\hat{y}$.  Let's look at the regression model we are fitting:

\vspace{.5in}

If we want to find a Confidence Interval for $\hat{y}$ what would need:

\vspace{.3in}

Let's take a 2000 sq. foot house that's not on the lakefront.  In order to find the variance of our prediction we need the variance of $\hat{\beta_0}$, the variance of $\hat{\beta_1}$ and the covariance between these two (recall the variance of a sum is the sum of the covariances).  In R we can get the covariance matrix from `vcov(inter.lm)`.  So we note that the variance here is:

```{r}
pred.var=5127.43+2700^2*.000801+2*2700*-1.94454
pred.se=sqrt(pred.var)
pred.se
```

 Using matrix algebra we can compute this as:

\vspace{.5in}

For any given observation, we can get the confidence intervals and the SE using:

```{r}
conf.int=predict(inter.lm,data.frame(sqft=2700,lake="lakefront"),se.fit=TRUE,interval="confidence")
```

If we want a prediction interval, we need the variance associated with $\hat{y}$ AND the variance associated with $y$, which is $\hat{\sigma^2}$.  So for instance if we want a prediction of a future lakefront house that's 2700, our variance will be `21.62^2+49.48^2`.  So the SE for a prediction will be approx. 54.  Our book makes the claim that to form a 95\% PI we should use $\hat{y} \pm 2* \hat{\sigma^2}$ which fails to account for the uncertainty in $\hat{\beta}$ terms....

To find a prediction interval we can use:

```{r}
pred.int=predict(inter.lm,data.frame(sqft=2700,lake="lakefront"),se.fit=TRUE,interval="prediction")
conf.int$fit
pred.int$fit  
```