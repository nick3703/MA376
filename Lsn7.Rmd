---
title: "Lesson 7"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Admin


Putting it all together.

A British study (North, Shilcock, Hargreaves, 2003) examined whether the type of background music playing in a restaurant ffected the amount of money that diners spent on their meals.  The researchers asked a restaurant to alternate silence, popular music, and classical music on successive nights over 18 days.  Each type of music was played for six nights. 

What are the researchers research question?

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
music.dat<-read.csv("Music.csv")
music.dat %>% ggplot(aes(x=MusicType,y=Amount))+geom_boxplot()

```

Can we make a conclusion about the research question based on this plot?

\vspace{.4in}

What is our sources of variation diagram?

\vspace{1.in}

What is our statistical model?

\vspace{1.in}

What is an appropriate statistic to use?

\vspace{1.in}

Are our validity conditions met?

\vspace{.8in}

Assuming yes, what is the distribution of our F Statistic?

\vspace{.8in}

What are our conclusions?  Do they help the restaurant?

```{r}
contrasts(music.dat$MusicType)=contr.treatment
lm.mod<-lm(Amount~MusicType,data=music.dat)
anova(lm.mod)
```



Perhaps we want to say something more.  In this case we might want to conduct a \textbf{Post Hoc Test}.  Note that this is only permissible IF our F statistic is statistically significant.  Why?

\vspace{.8in}

Our book forms the pairwise difference using the formula \textit{difference in means}$\pm$(multiplier)(SE residuals x $\sqrt{1/n_1+1/n_2}$).  It is obvious what $n_1$ and $n_2$ are, SE residuals is our estimate of $\sigma$ that is found from SSE/df  where df = $n-1-(groups-1)=n-groups$.  This can be found in the Residual standard error line of our lm output

```{r}
contrasts(music.dat$MusicType)=contr.sum
music.lm<-lm(Amount~MusicType,data=music.dat)
summary(music.lm)
```
So here SE residual is 2.85

Our book then forms each of the pairwise differences.  The multiplier is roughly 2 for a 95 \% CI (How do I know this?)

```{r}
music.dat%>%group_by(MusicType)%>%summarise(avg=mean(Amount),num=n())

(21.9-21.7)+2*2.85*sqrt(1/142+1/131)

(21.9-21.7)-2*2.85*sqrt(1/142+1/131)
```


This method is not generally the best though because our overall Type I error rate, or the experiment (or family wise) Type I error rate will not be $\alpha=.05$.  Each individual confidence interval will be 95 \% meaning if there's a 5 percent chance I've made an error for each contrast and I do 3 contrasts the overall probability I've made an error is:

\vspace{.8in}

There's other methods such as Bonferronni corrections you might have heard about (or might here about) but R has a built in way to correct that changes the multiplier above.  This method is called Tukey's Honest Significant Difference or TukeyHSD.  It's a bit weird, but what we have to do is:

```{r}
aov.obj<-aov(music.lm)
TukeyHSD(aov.obj)
```
From here we can make a letters table:


\vspace{.8in}

We can also find CIs for each individual mean.  This can be done in R but we have to be very very careful.

```{r}
contrasts(music.dat$MusicType)=contr.sum
music.lm<-lm(Amount~MusicType,data=music.dat)
confint(music.lm)
music.lm2<-lm(Amount~0+MusicType,data=music.dat)
confint(music.lm2)
```

In terms of our parameters,  what is going on here?  What one is right?

\vspace{.8in}

To check we can again use the formula for our book $\bar{y_i }\pm t_{\alpha/2,n-1}\frac{\hat{\sigma}}{\sqrt{n_i}}$.  

\vspace{.8in}

What, in this formula, changes as $n$ gets bigger?  What does this mean for our confidence interval?

\vspace{.8in}

In terms of our statistical model a confidence interval for the mean is a confidence interval for what?

\vspace{.8in}

A prediction interval would be an interval for what?

\vspace{.8in}

So if we want to form a prediction interval for a new observation can we use $\bar{y_i }\pm \frac{\hat{\sigma}}{\sqrt{n_i}}$?

\vspace{.8in}

Where is our uncertainty coming from if we are making a prediction?


\vspace{.8in}

What happens to this interval as $n_i \to \infty$?

\vspace{.8in}

If we want to form a prediction interval we want to be reasonably sure we have captured the possible values of $y_{new}$.  Obviously if $y$ is in our data set we know the value so we wouldn't want to predict.  In terms of our problem we would want to be able to say, with some level of certainty, not what the average tip would be, but rather how much the next person in the door will tip our staff.

What can we tell our restauranteer?  If they play classical music give a prediction for tip they will receive.  

\vspace{.8in}



Note the validity conditions on page 110 for prediction intervals.  Even if our validity conditions are met for using the F-test we might not meet the validity conditions for prediction intervals.  Why?
