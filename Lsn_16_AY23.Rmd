---
title: "Lsn 16 AY23"
author: "Clark"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Reading Critique, Project, WPR...

Up to this point we have been using statistical models of the form:

\vspace{1.in}

Key to this has been the assumption that our explanatory variable, or independent variable, is categorical.  This was nice in that we could think of each of our observations as coming from a group with separate means.  For instance we can think of $H_0$ and $H_a$ from above as:

\vspace{1.in}

However, in some studies our explained variation comes from a variable that has a natural ordering to it.  In fact we've seen this a bit already (recall Pistachio study).  

Grape Seeds:  

Note here our researchers are interested in explaining the variation in the amount of proanthocyanidin (PC) in a grape seed and the sources of the explained variation might be thought of as the percentage of ethanol.

```{r}
grape<-read.table("http://www.isi-stats.com/isi2/data/Polyphenols.txt",header=T, stringsAsFactors = T)
grape <- grape %>% mutate(Ethanol=Ethanol.) %>% select(-Ethanol.)
grape <- grape %>% mutate(Time.hrs=Time.hrs.)%>% select(-Time.hrs.)
```

Note that the null model here is:

\vspace{1.in}

Which can be fit as:
```{r}
null.lm<-lm(PC~1,data=grape)
summary(null.lm)
```

Perhaps we fit a separate means model:

\vspace{1.in}

Which is done through:

```{r}
sep.means<-lm(PC~0+as.factor(Ethanol),data=grape)
summary(sep.means)
```
Is this better?  Well, perhaps.  To test this we can run:

```{r}
grape<-grape %>% mutate(Ethanolf=as.factor(Ethanol))
contrasts(grape$Ethanolf)<-contr.sum
effects.mod<-lm(PC~Ethanolf,data=grape)
anova(effects.mod)
```

\vspace{1.in}

```{r}
gr.means <- grape %>% group_by(Ethanolf)%>%summarize(means=mean(PC))%>%
  mutate(Ethanol=as.numeric(as.character(Ethanolf)))
grape %>% ggplot(aes(x=Ethanol,y=PC))+geom_point()+geom_point(aes(x=Ethanol,y=means),color="red",size=4,data=gr.means)
```

If our means have a linear relationship, perhaps our model could be:

\vspace{1.in}

Which is of course the model for a regression.  Note that this is the statistical model.  The fitted model or the predicted model is:

\vspace{1.in}

Our book uses $b_0$ and $b_1$ instead of $\hat{\beta}_0$ and $\hat{\beta}_1$.  I prefer the hats, but use what you'd like.

To fit this model we simply run:

```{r}
reg.lm<-lm(PC~Ethanol,data=grape)
summary(reg.lm)
```

Note that we obtain $\hat{\beta}$ through the method of least squares:

\vspace{1.in}

The interpretation of $\hat{\beta}_0$ and $\hat{\beta_1}$ are:

One often overlooked aspect of using regression vs ANOVA is that the linear regression model is actually more restrictive than the seperate means model:

```{r}
anova(reg.lm)
```

Note that the residual sums of squares are higher under this model.  Why?

\vspace{.5in}

However, the MSE of the residuals is smaller.  When we use a regression model vs a separate means model we are making a tradeoff.  We are actually building a simpler model, but hoping that the additional complexity a multiple means model provides minimal difference in explaining variability.  \textbf{All things being equal we always prefer a simpler model}.  If we have two models that have similar SSError values, choose the model that uses fewer degrees of freedom if you don't have science to save you.

To put this another way, previously if we had $\mu$, $\alpha_1$ and $\alpha_2$ we automatically knew $\alpha_3$ (why?).  Now, if we know one of our means and $\beta_1$ we know all of our other means.

```{r}
grape %>% ggplot(aes(x=Ethanol,y=PC))+geom_point()+
  geom_point(aes(x=Ethanol,y=means),color="red",size=4,data=gr.means)+
  stat_smooth(method="lm", se=FALSE)
```

The key point here is that a linear regression model is appropriate if we believe there is a linear relationship between our explanatory variable and our response.  It isn't always clear whether a separate means model outperforms a linear regression model though:


```{r}
grape<-grape %>% mutate(Timef=as.factor(Time.hrs))
reg.lm<-lm(PC~Time.hrs,data=grape)
sep.means<-lm(PC~Timef,data=grape)
```


```{r}

gr.means <- grape %>% group_by(Timef)%>%summarize(means=mean(PC))%>%
  mutate(Time=as.numeric(as.character(Timef)))
grape %>% ggplot(aes(x=Time.hrs,y=PC))+geom_point()+
  geom_point(aes(x=Time,y=means),color="red",size=4,data=gr.means)+
  stat_smooth(method="lm",se=FALSE)
```


if we have a scientific reason for believing that Ethanol is linearly related to PC than it certainly would be advantageous to use a linear regresison model over a group means (or cell means) model.  Note that our $H_0$ and $H_a$ can be written in words as:

\vspace{1.in}

Another way to think about this set of hypothesis (hypotheses?) is that we are testing whether the linear regression model explains more variation (statistically speaking) than the null model.  Recall that the null model is:

\vspace{1.in}

One statistic that our book starts with that can be used to test this hypothesis is our $R^2$ statistic.  Remember that $R^2$ is a measurement of how much of our variation can be explained through our explanatory variables.  \textbf{Note that we are NOT (at least now) comparing the linear regression model to the group means model, we are only comparing the linear regression model to the null model.}

So, we have our $H_0$ and $H_a$, we have our test statistic, now from our data we can calculate the observed statistic.

```{r}
reg.lm<-lm(PC~Ethanol,data=grape)
our.stat<-summary(reg.lm)$'r.squared'
```

We then can see how rare it would be, if Ethanol didn't matter, that we would have observed \textit{our} $R^2$.

```{r}
M<-1000
empirical.dist<-data.frame(trial=seq(1,M),stat=NA)
for(i in 1:M){
  grape.shuff<-grape %>% mutate(Ethanol.shuff=sample(Ethanol))
  shuff.lm<-lm(PC~Ethanol.shuff,data=grape.shuff)
  empirical.dist[i,]$stat=summary(shuff.lm)$'r.squared'
}
```

So how rare is our statistic?

```{r}
empirical.dist %>% ggplot(aes(x=stat))+
  geom_histogram()+geom_vline(xintercept=our.stat,color="red",lwd=2)
empirical.dist%>%filter(stat>our.stat)%>%summarize(pval=n()/M)
```

This is all well and good, but the distribution of $R^2$ changes depending on our data (why?).  So perhaps this isn't the best statistic to use.

Note that from our model a test comparing the null model to the linear regression model is also a test of:

\vspace{.5in}

Therefore, it might make sense to use $\hat{\beta_1}$ as our test statistic.  It just so happens that, assuming our validity conditions are met, we \textbf{know} the distribution of $\hat{\beta_1}$.  The validity conditions are LINE (cute, huh...).  Of these, the ones that really matter are outliers and independence (in my opinion), we are relatively robust otherwise.  The validity conditions can be wrapped up into $\epsilon_{i,j}$ as:

\vspace{.5in}

We can never actually oberve $\epsilon_{i,j}$ but we can estimate it from $r_{i,j}=y_{i,j}-\hat{y}_{i,j}$ where $\hat{y}_{i,j}$ is found from:

\vspace{.5in}

This allows us to check Normality and equal variance.  To check linearity we can plot the residuals vs. predicted values:

```{r}
resids=reg.lm$residuals
yhat=reg.lm$fitted.values
qqnorm(resids)
qqline(resids)
#hist(resids)
#plot(yhat,resids)
```

If our conditions are met, then we can form the standardized statistic, which has distribution:

\vspace{1.in}

But wait a minute...  What about ANOVA? Isn't that the framework we were using to test $\alpha_1=\alpha_2=\alpha_3$?

```{r}
anova(reg.lm)
```

As it turns out, an F-statistic with 1,n degrees of freedom is the square of a t-statistic with n degrees of freedom.  So, if we look at our ANOVA output, we have an F statistic of 3.79 that has 1,13 degrees of freedom, which yields a p-values of `1-pf(3.79,1,13)=0.073`, but we can do the same test by testing $\beta_1=0$ vs $\beta_1 \neq 0$ using $\hat{\beta}_1$ which has a t distribution with 13 degrees of freedom.  A picture:

\vspace{1.5in}

The goodness of using $\hat{\beta}_1$ vs the F statistic is that the Confidence interval for $\beta_1$ that can be formed using $\hat{\beta}_1$ is more intuitive.  Recall that the general form of a CI is:

\vspace{1.in}

Therefore we need the SE of $\hat{\beta}_1$.  Using matrix notation the SE is trivial to find.  Outside of the world of linear algebra it's cumbersome and not necessarily that insightful (learn Linear Algebra!!)

However, practically in R we can do:

```{r}
confint(reg.lm,level=0.95)
```

So our 95\% CI for $\hat{\beta}_1$ is:

